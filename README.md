# Project Overview
This project demonstrates an end-to-end data engineering workflow using a retail transactions dataset.
The goal is to ingest, clean, transform, and analyze retail transaction data to generate actionable business insights using Apache Spark (PySpark).

The project follows data engineering best practices, including:
- Schema validation
- Data transformations
- Analytical aggregations

# Problem Statement
Large volumes of transactional data are gotten from retail shops daily. However, some data don't support business decisions hence this project

Key challenges addressed:
- Inconsistent schemas and data types
- Poor data quality (string-based numeric fields, missing values, errors)
- Difficulty answering or understanding customer behaviour
This project solves these challenges by building a scalable Spark-based pipeline that converts raw data into cleaned data and answering some business questions.

# Data Source
The dataset represents anonymized retail transaction records containing items bought, customer name, total cost of items simualating real-world shopping data.
Link to Data - https://drive.google.com/file/d/16WkFI0muZSP0sya4G7McuLsMxYPxog7P/view?usp=sharing

# Data Flow:
Raw Data → PySpark Transformations → business questions answered

# Transformation Logic
The pipeline performs the following transformations:

- Schema Enforcement
- Data Cleaning
- Data Standardization (changed column name to snake_case)
- Transaction timestamp splitted to transaction time and transaction hour
- Currency values are normalized
- Aggregation
- Customer-level metrics such as total spend and transaction count are computed

# Business Questions
- Top 10 purchased products
- How discount impact sales
- Revenue generated by store type
- Revenue generated by city
- Peak buying hour
- Customer buying metrics
- Sales performance by season

# How to Run Project
spark-submit /RetailProject.py

# Tech Stack
- Apache Spark (PySpark)
- Git & GitHub
- Linux environment
- Google collab

# Key Skills Demonstrated
- End-to-end data pipeline design
- PySpark data processing
- Data quality and schema enforcement

# Future Improvements
- Airflow orchestration
- Advanced Data quality checks
- Incremental data processing
- Unit testing for transformation
- Cloud deployment (S3 + EMR / Databricks)
- BI integration (Power BI / Tableau)

# Author
Love Elusogbon - Budding Data Engineer
